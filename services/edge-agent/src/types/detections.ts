/**
 * Detection and Frame Metadata Types
 *
 * ═══════════════════════════════════════════════════════════════════════════════
 * PURPOSE
 * ═══════════════════════════════════════════════════════════════════════════════
 *
 * Shared type definitions for object detection results and video frame metadata.
 * Single source of truth for detection data structures used across AI, Store,
 * and Orchestrator modules.
 *
 * ═══════════════════════════════════════════════════════════════════════════════
 * ARCHITECTURE
 * ═══════════════════════════════════════════════════════════════════════════════
 *
 * Type hierarchy:
 *
 * 1. BoundingBox
 *    - Spatial coordinates of detected object
 *    - Format: [x, y, width, height]
 *    - Used by: Detection type
 *
 * 2. Detection
 *    - Single object detection result
 *    - Includes: class, confidence, bounding box, optional track ID
 *    - Used by: ai.detection events, Session Store ingestion
 *
 * 3. FrameMeta
 *    - Video frame metadata (timestamp, dimensions, pixel format)
 *    - Used by: Frame ingestion, format conversion, frame caching
 *
 * ═══════════════════════════════════════════════════════════════════════════════
 * BOUNDING BOX FORMAT
 * ═══════════════════════════════════════════════════════════════════════════════
 *
 * Tuple format: [x, y, width, height]
 *
 * - x: Left edge coordinate (pixels from left)
 * - y: Top edge coordinate (pixels from top)
 * - width: Bounding box width (pixels)
 * - height: Bounding box height (pixels)
 *
 * Coordinate system:
 * - Origin (0, 0) is top-left corner of frame
 * - x increases to the right
 * - y increases downward
 *
 * Example:
 * ```
 * Frame (1920×1080)
 * ┌─────────────────────────────────────┐
 * │ (0,0)                               │
 * │                                     │
 * │         ┌───────────┐               │
 * │         │  Person   │ ← bbox: [500, 300, 200, 400]
 * │         │  (x=500,  │               │
 * │         │   y=300,  │               │
 * │         │   w=200,  │               │
 * │         │   h=400)  │               │
 * │         └───────────┘               │
 * │                                     │
 * │                          (1920,1080)│
 * └─────────────────────────────────────┘
 * ```
 *
 * Why tuple instead of object?
 * - Compact representation (less memory)
 * - Fast serialization (JSON arrays are efficient)
 * - Compatible with NumPy/OpenCV conventions (common in AI)
 *
 * ═══════════════════════════════════════════════════════════════════════════════
 * DETECTION TYPE
 * ═══════════════════════════════════════════════════════════════════════════════
 *
 * Single object detection result from AI model.
 *
 * Fields:
 *
 * - cls: Object class name (e.g., "person", "bottle", "shoes")
 *   - From the model's class catalog (custom fine-tuned dataset)
 *   - Used for filtering relevant vs irrelevant detections
 *   - Example classes: backpack, bottle, cup, person, shoes
 *
 * - conf: Detection confidence score (0.0 to 1.0)
 *   - Model's certainty that object is present
 *   - Higher = more confident
 *   - Typical threshold: 0.5 (50% confidence)
 *   - Used for filtering low-confidence false positives
 *
 * - bbox: Bounding box coordinates [x, y, width, height]
 *   - Spatial location of detected object
 *   - See BoundingBox documentation above
 *
 * - trackId: Optional object tracking ID (for multi-frame tracking)
 *   - Stable ID across multiple frames (e.g., "person_17")
 *   - Enables tracking same object over time
 *   - Generated by tracking algorithms (SORT, DeepSORT, ByteTrack)
 *   - undefined if tracking is disabled
 *
 * Example detection:
 * ```typescript
 * {
 *   cls: "person",
 *   conf: 0.87,
 *   bbox: [500, 300, 200, 400],
 *   trackId: "person_17"
 * }
 * // → "Person detected at (500, 300) with size 200×400, 87% confidence, tracked as person_17"
 * ```
 *
 * ═══════════════════════════════════════════════════════════════════════════════
 * FRAME METADATA TYPE
 * ═══════════════════════════════════════════════════════════════════════════════
 *
 * Video frame metadata (without pixel data).
 *
 * Fields:
 *
 * - ts: Frame timestamp (ISO 8601 string)
 *   - Format: "2024-01-15T10:30:45.123Z"
 *   - UTC timezone
 *   - Used for: Frame ordering, detection correlation, session timeline
 *
 * - width: Frame width in pixels (e.g., 1920)
 *   - Must match actual pixel buffer width
 *   - Used for: Bounding box validation, format conversion
 *
 * - height: Frame height in pixels (e.g., 1080)
 *   - Must match actual pixel buffer height
 *   - Used for: Bounding box validation, format conversion
 *
 * - pixFmt: Pixel format (RGB | I420 | NV12)
 *   - RGB: 24-bit color (3 bytes per pixel), good for display
 *   - I420: YUV 4:2:0 planar (1.5 bytes per pixel), good for video encoding
 *   - NV12: YUV 4:2:0 semi-planar (1.5 bytes per pixel), GPU-friendly
 *   - Used for: Format conversion, buffer size validation
 *
 * Example metadata:
 * ```typescript
 * {
 *   ts: "2024-01-15T10:30:45.123Z",
 *   width: 1920,
 *   height: 1080,
 *   pixFmt: "NV12"
 * }
 * // → 1920×1080 NV12 frame captured at 10:30:45.123 UTC
 * ```
 *
 * ═══════════════════════════════════════════════════════════════════════════════
 * USAGE EXAMPLES
 * ═══════════════════════════════════════════════════════════════════════════════
 *
 * ```typescript
 * import type { Detection, FrameMeta, BoundingBox } from "./types/detections.js";
 *
 * // Example 1: AI detection event
 * const detection: Detection = {
 *   cls: "person",
 *   conf: 0.87,
 *   bbox: [500, 300, 200, 400],
 *   trackId: "person_17"
 * };
 *
 * bus.publish("ai.detection", {
 *   type: "ai.detection",
 *   detections: [detection],
 *   relevant: true, // Person is relevant class
 * });
 *
 * // Example 2: Frame metadata for ingestion
 * const frameMeta: FrameMeta = {
 *   ts: new Date().toISOString(),
 *   width: 1920,
 *   height: 1080,
 *   pixFmt: "NV12"
 * };
 *
 * await frameIngester.ingest({
 *   sessionId: "sess_cam-01_1234567890_1",
 *   meta: frameMeta,
 *   detections: [detection],
 *   jpegBuffer: compressedFrame,
 * });
 *
 * // Example 3: Bounding box validation
 * function isValidBbox(bbox: BoundingBox, frame: FrameMeta): boolean {
 *   const [x, y, w, h] = bbox;
 *   return (
 *     x >= 0 && y >= 0 &&
 *     x + w <= frame.width &&
 *     y + h <= frame.height
 *   );
 * }
 *
 * // Example 4: Class filtering (relevant detection)
 * const RELEVANT_CLASSES = ["person", "car", "truck"];
 * const isRelevant = RELEVANT_CLASSES.includes(detection.cls);
 * ```
 *
 * ═══════════════════════════════════════════════════════════════════════════════
 * INTEGRATION POINTS
 * ═══════════════════════════════════════════════════════════════════════════════
 *
 * Used by:
 * - AIClient: Parses detections from AI worker (Protocol Buffer → Detection)
 * - FrameIngester: Uploads frames + detections to Session Store
 * - FSM: Filters relevant detections (triggers state transitions)
 * - FrameCache: Stores frame metadata for temporal correlation
 * - Convert module: Uses FrameMeta for format conversion
 *
 * Dependencies:
 * - None (pure type definitions, no runtime dependencies)
 *
 * @module types/detections
 */

/**
 * Bounding Box - Spatial coordinates of detected object
 *
 * Tuple format: [x, y, width, height]
 *
 * - x: Left edge (pixels from left, 0-based)
 * - y: Top edge (pixels from top, 0-based)
 * - width: Box width (pixels)
 * - height: Box height (pixels)
 *
 * Coordinate system: Top-left origin (0, 0), x right, y down
 *
 * @example
 * ```typescript
 * const bbox: BoundingBox = [500, 300, 200, 400];
 * // → Object at (500, 300) with size 200×400 pixels
 * ```
 */
export type BoundingBox = [x: number, y: number, w: number, h: number];

/**
 * Detection - Single object detection result
 *
 * Contains class name, confidence score, bounding box, and optional track ID.
 *
 * @property cls - Object class name (e.g., "person", "car", "dog")
 * @property conf - Confidence score (0.0 to 1.0, higher = more confident)
 * @property bbox - Bounding box [x, y, width, height]
 * @property trackId - Optional tracking ID for multi-frame tracking
 *
 * @example
 * ```typescript
 * const detection: Detection = {
 *   cls: "person",
 *   conf: 0.87,
 *   bbox: [500, 300, 200, 400],
 *   trackId: "person_17"
 * };
 * ```
 */
export type Detection = {
  cls: string;
  conf: number;
  bbox: BoundingBox;
  trackId?: string;
};

/**
 * Frame Metadata - Video frame information (without pixel data)
 *
 * Contains timestamp, dimensions, and pixel format.
 * Used for frame ingestion, conversion, and validation.
 *
 * @property ts - Frame timestamp (ISO 8601 UTC string)
 * @property width - Frame width in pixels
 * @property height - Frame height in pixels
 * @property pixFmt - Pixel format (RGB | I420 | NV12)
 *
 * Pixel formats:
 * - RGB: 24-bit color (3 bytes/pixel), display-friendly
 * - I420: YUV 4:2:0 planar (1.5 bytes/pixel), video encoding
 * - NV12: YUV 4:2:0 semi-planar (1.5 bytes/pixel), GPU-friendly
 *
 * @example
 * ```typescript
 * const meta: FrameMeta = {
 *   ts: "2024-01-15T10:30:45.123Z",
 *   width: 1920,
 *   height: 1080,
 *   pixFmt: "NV12"
 * };
 * ```
 */
export type FrameMeta = {
  ts: string; // ISO 8601 timestamp (UTC)
  width: number;
  height: number;
  pixFmt: "RGB" | "I420" | "NV12";
};
