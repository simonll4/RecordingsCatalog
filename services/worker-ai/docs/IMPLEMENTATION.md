# Worker AI - ImplementaciÃ³n Completa

## âœ… Objetivo Cumplido

Se ha implementado exitosamente la separaciÃ³n del worker de IA en su propio contenedor Python con comunicaciÃ³n TCP + Protobuf, manteniendo la semÃ¡ntica del orquestador existente.

## ðŸ“‹ Componentes Implementados

### 1. Contrato Protobuf (`proto/ai.proto`)

âœ… **Mensajes definidos:**
- `Envelope` (wrapper con oneof)
- `Request` â†’ `Init`, `Frame`, `End`
- `End` cierra la sesiÃ³n activa en el worker y fuerza el cierre de archivos JSON.
- `Response` â†’ `InitOk`, `Ready`, `Result`, `Error`
- `Heartbeat` (bidireccional)

âœ… **Campos clave:**
- `model_id`: hash del modelo cargado
- `ts_mono_ns`: timestamp monotÃ³nico para sincronizaciÃ³n
- `max_frame_bytes`: lÃ­mite de payload
- `providers`: execution providers (CPU/CUDA)
- `preproc`: info de preprocesamiento

### 2. Worker Python (`services/worker-ai/`)

âœ… **CaracterÃ­sticas:**
- Estados: IDLE, LOADING, READY, SESSION_ACTIVE
- Regla de desconexiÃ³n â†’ timer de inactividad (60s default)
- Hot-reload: Init diferente â†’ recarga modelo
- Ventana 1: backpressure natural
- ONNX Runtime con soporte CPU/CUDA
- YOLO11 con NMS optimizado

âœ… **Archivos:**
- `worker_new.py`: Servidor TCP asyncio
- `requirements.txt`: numpy, onnxruntime, protobuf
- `Dockerfile`: Image Python 3.11-slim
- `healthcheck.py`: Health check TCP
- `README.md`: DocumentaciÃ³n completa

### 3. Cliente TCP Node (`services/edge-agent/src/modules/ai-client.ts`)

âœ… **CaracterÃ­sticas:**
- Backpressure ventana 1 (Ready/Result)
- Latest-wins: reemplaza frame encolado
- ReconexiÃ³n automÃ¡tica con backoff exponencial
- Re-init al reconectar
- Heartbeat timeout (10s)
- Length-prefixed framing (uint32LE)

âœ… **Interfaz estable:**
```typescript
interface AIClient {
  connect(): Promise<void>;
  init(args): Promise<void>;
  canSend(): boolean;
  sendFrame(...): void;
  onResult(cb): void;
  shutdown(): Promise<void>;
}
```

### 4. AI Engine TCP (`services/edge-agent/src/modules/ai-engine-tcp.ts`)

âœ… **CaracterÃ­sticas:**
- Interfaz `AIEngine` compatible con orquestador
- Delega inferencia al worker
- Filtra detecciones (umbral + clases) en Node
- Publica eventos `ai.detection` / `ai.keepalive`
- Keepalive automÃ¡tico (2s)

### 5. IntegraciÃ³n con Orquestador

âœ… **Sin cambios en:**
- `Orchestrator`: FSM sin modificaciones
- `AICapture`: sigue usando `onFrame` callback
- Eventos del bus: misma semÃ¡ntica
- Estados FSM: IDLE â†’ DWELL â†’ ACTIVE â†’ CLOSING

âœ… **Cambios mÃ­nimos en:**
- `main.ts`: `AIEngineSim` â†’ `AIEngineTcp`
- `config/schema.ts`: campo `worker: { host, port }`
- `config/index.ts`: lectura de `AI_WORKER_HOST/PORT`

### 6. Docker Compose

âœ… **Nuevo servicio:**
```yaml
worker-ai:
  build: ./services/worker-ai
  environment:
    - BIND_HOST=0.0.0.0
    - BIND_PORT=7001
    - IDLE_TIMEOUT_SEC=60
  volumes:
    - ./data/models:/models:ro
  healthcheck:
    test: ["CMD", "python3", "healthcheck.py"]
```

âœ… **Actualizado:**
```yaml
edge-agent:
  depends_on:
    worker-ai:
      condition: service_healthy
  environment:
    - AI_WORKER_HOST=worker-ai
    - AI_WORKER_PORT=7001
```

### 7. Scripts y Herramientas

âœ… **Scripts creados:**
- `scripts/generate-proto.sh`: Genera cÃ³digo Protobuf (Python + TS)
- `scripts/setup-ai-worker.sh`: Setup completo (deps + proto + modelo)

âœ… **DocumentaciÃ³n:**
- `services/worker-ai/README.md`: Arquitectura, API, troubleshooting
- `docs/AI_WORKER_MIGRATION.md`: GuÃ­a de migraciÃ³n
- `data/models/README.md`: CÃ³mo obtener modelos ONNX

## ðŸŽ¯ Criterios de AceptaciÃ³n

### Unit Tests
- âœ… Parser de framing length-prefixed
- âœ… MÃ¡quina de estados Ready/Result
- âœ… Latest-wins (encolado)

### Integration Tests
- âœ… Handshake Init/InitOk/Ready
- âœ… EnvÃ­o de frames sintÃ©ticos
- âœ… RecepciÃ³n de Result
- âœ… PublicaciÃ³n de eventos al bus

### Resiliencia
- âœ… ReconexiÃ³n automÃ¡tica
- âœ… Idle timeout â†’ unload modelo
- âœ… Re-init al reconectar
- âœ… Hot-reload de modelo

### Rendimiento (objetivo)
- ðŸŽ¯ 640Ã—480 RGB @ 10-12 fps
- ðŸŽ¯ p95 end-to-end < 150ms
- ðŸŽ¯ Drops por latest-wins â‰¤ 5%

## ðŸ“Š MÃ©tricas Implementadas

### Agent (Node)
- `ai_init_ok_total`
- `ai_reconnects_total`
- `ai_frame_inflight` (0/1)
- `ai_frames_sent_total`
- `ai_drops_latestwins_total`
- `ai_result_latency_ms`
- `ai_detections_total`

### Worker (Python)
- `worker_state` (IDLE/LOADING/READY)
- `connections_active`
- `frames_processed_total`
- `inference_time_ms`
- `model_load_seconds`
- `transitions_to_idle_total`

## ðŸš€ CÃ³mo Empezar

### 1. Setup Inicial
```bash
./scripts/setup-ai-worker.sh
```

### 2. Arrancar Servicios
```bash
docker-compose up -d
```

### 3. Verificar
```bash
# Logs
docker-compose logs -f worker-ai edge-agent

# Salud
docker-compose ps

# MÃ©tricas
docker stats worker-ai edge-agent
```

### 4. Testing
```bash
# Unit tests
cd services/edge-agent
npm test

# Integration tests
npm run test:integration
```

## ðŸ”§ ConfiguraciÃ³n

### Variables CrÃ­ticas

**Worker:**
- `BIND_HOST`: 0.0.0.0
- `BIND_PORT`: 7001
- `IDLE_TIMEOUT_SEC`: 60

**Agent:**
- `AI_WORKER_HOST`: worker-ai
- `AI_WORKER_PORT`: 7001
- `AI_MODEL_NAME`: /models/yolo11s.onnx
- `AI_UMBRAL`: 0.35
- `AI_FPS_IDLE`: 5
- `AI_FPS_ACTIVE`: 12

## ðŸ“¦ Estructura Final

```
tpfinal-v3/
â”œâ”€â”€ proto/
â”‚   â””â”€â”€ ai.proto                    # Contrato Ãºnico
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ edge-agent/
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ai-client.ts       # TCP client
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ai-engine-tcp.ts   # Engine remoto
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ai-engine-sim.ts   # Simulador (dev)
â”‚   â”‚   â”‚   â””â”€â”€ proto/
â”‚   â”‚   â”‚       â”œâ”€â”€ ai_pb.js           # Generado
â”‚   â”‚   â”‚       â””â”€â”€ ai_pb.d.ts         # Generado
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â””â”€â”€ worker-ai/
â”‚       â”œâ”€â”€ worker_new.py              # Worker Python
â”‚       â”œâ”€â”€ ai_pb2.py                  # Generado
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ healthcheck.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ yolo11s.onnx              # Modelo ONNX
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate-proto.sh
â”‚   â””â”€â”€ setup-ai-worker.sh
â””â”€â”€ docker-compose.yml
```

## ðŸ› Troubleshooting ComÃºn

### Worker no arranca
```bash
docker-compose logs worker-ai
docker-compose exec worker-ai ls -lh /models/
```

### Agent no conecta
```bash
docker-compose exec edge-agent nc -zv worker-ai 7001
docker-compose exec edge-agent env | grep AI_WORKER
```

### Modelo no carga
```bash
docker-compose exec worker-ai python3 -c "
import onnxruntime as ort
session = ort.InferenceSession('/models/yolo11s.onnx')
print('OK')
"
```

## ðŸŽ“ PrÃ³ximos Pasos

1. â³ Tests de carga (stress testing)
2. â³ MÃ©tricas avanzadas (Prometheus)
3. â³ GPU support (CUDA/TensorRT)
4. â³ Batch inference
5. â³ Model zoo (mÃºltiples modelos)
6. â³ Compression (JPEG/WebP)
7. â³ Multi-client support

## ðŸ“š Referencias

- [Protobuf Docs](https://protobuf.dev/)
- [ONNX Runtime](https://onnxruntime.ai/)
- [YOLO11](https://github.com/ultralytics/ultralytics)
- [TCP Backpressure](https://ferd.ca/queues-don-t-fix-overload.html)

---

**Estado:** âœ… ImplementaciÃ³n completa y funcional  
**Fecha:** Octubre 2025  
**Autor:** Edge Agent Team
